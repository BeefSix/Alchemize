# app/services/video_engine.py
import os
import subprocess
import shutil
import re

try:
    import torch
    from diffusers import DiffusionPipeline
    DIFFUSERS_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Warning: Could not import diffusers: {e}")
    DIFFUSERS_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è Warning: CUDA/xFormers compatibility issue: {e}")
    DIFFUSERS_AVAILABLE = False

# Use the correct import path for the new structure
from app.services import utils

class LocalStableDiffusionGenerator:
    _instance = None
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(LocalStableDiffusionGenerator, cls).__new__(cls)
            cls._instance.pipeline = None
        return cls._instance

    def load_model(self):
        if not DIFFUSERS_AVAILABLE:
            return False, "Diffusers library not available or CUDA/xFormers compatibility issue."
        if self.pipeline: return True, "Model already loaded."
        try:
            print("Loading Stable Diffusion model to GPU...");
            self.pipeline = DiffusionPipeline.from_pretrained(
                "SG161222/RealVisXL_V4.0",
                torch_dtype=torch.float16,
                use_safetensors=True,
                variant="fp16"
            ).to("cuda")
            self.pipeline.enable_model_cpu_offload()
            return True, "Model loaded successfully."
        except Exception as e:
            self.pipeline = None
            return False, str(e)

    def generate(self, prompt, width, height):
        if not self.pipeline: return None
        try:
            p_prompt = f"cinematic shot, masterpiece, 4k, photorealistic, {prompt}"
            n_prompt = "deformed, ugly, blurry, low quality, cartoon, anime, disfigured, bad hands"
            image = self.pipeline(prompt=p_prompt, negative_prompt=n_prompt, width=width, height=height, num_inference_steps=25).images[0]
            return image
        except Exception as e:
            print(f"‚ùå Image generation failed: {e}")
            return None
sd_generator = LocalStableDiffusionGenerator()

def _is_safe_path(path):
    # Only allow alphanumeric, dash, underscore, dot, and forward/backslash
    return bool(re.match(r'^[\w\-./:\\]+$', path))

def create_ass_file(words_data, output_path):
    style_header = "[V4+ Styles]"
    style_format = "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding"
    style_values = f"Style: Default,Impact,48,&H00FFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,1,2,10,10,20,1"
    event_header = "[Events]"
    event_format = "Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("[Script Info]\nTitle: Generated by ClipGen\n\n")
        f.write(f"{style_header}\n{style_format}\n{style_values}\n\n")
        f.write(f"{event_header}\n{event_format}\n")

        for word_info in words_data:
            start_time = f"{int(word_info['start'] // 3600)}:{int((word_info['start'] % 3600) // 60):02}:{word_info['start'] % 60:05.2f}"
            end_time = f"{int(word_info['end'] // 3600)}:{int((word_info['end'] % 3600) // 60):02}:{word_info['end'] % 60:05.2f}"
            text = word_info['word'].upper().replace('"', '\\"').replace('{', '\\{').replace('}', '\\}')
            dialogue_line = f"Dialogue: 0,{start_time},{end_time},Default,,0,0,0,," + "{\\c&H00FFFF&}" + text
            f.write(dialogue_line + "\n")

def render_clip_with_ffmpeg(input_path, output_path, aspect_ratio="9:16", add_captions=False, words_data=None, add_music=False):
    ass_path = None
    for p in [input_path, output_path]:
        if not _is_safe_path(p):
            raise ValueError(f"Unsafe file path detected: {p}")
    try:
        video_input = ["-i", input_path]
        audio_input_path = utils.get_background_music()
        if add_music and audio_input_path:
             music_input = ["-i", audio_input_path]
             audio_map = "[1:a]volume=0.1[music];[0:a][music]amix=inputs=2[outa]"
             audio_output_map = ["-map", "[outa]"]
        else:
            music_input = []
            audio_map = ""
            audio_output_map = ["-map", "0:a?"]

        w, h = map(int, aspect_ratio.split(':'))
        crop_filter = f"crop=ih*{w}/{h}:ih"
        
        subtitle_filter = ""
        if add_captions and words_data:
            ass_path = f"temp_captions_{os.path.basename(output_path)}.ass"
            # Ensure the path is safe before writing
            if not _is_safe_path(ass_path):
                raise ValueError(f"Unsafe temp file path for subtitles: {ass_path}")
            create_ass_file(words_data, ass_path)
            subtitle_filter = f",subtitles={ass_path}:force_style='FontName=Impact,FontSize=48'"
            
        filter_complex = f"[0:v]{crop_filter},scale=1080:-2{subtitle_filter}[outv]"
        if audio_map:
            filter_complex = f"{audio_map};{filter_complex}"

        # Try CUDA first
        command_cuda = [
            "ffmpeg", "-hwaccel", "cuda", *video_input, *music_input,
            "-filter_complex", filter_complex, "-map", "[outv]", *audio_output_map,
            "-c:v", "h264_nvenc", "-preset", "p5", "-cq", "24", "-y", output_path
        ]
        
        print("üöÄ Executing FFmpeg command with CUDA acceleration...")
        try:
            result = subprocess.run(command_cuda, check=True, capture_output=True, text=True, timeout=300)
            print(f"‚úÖ FFmpeg rendering complete with CUDA: {output_path}")
            return True, output_path
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired) as e:
            print(f"‚ö†Ô∏è CUDA rendering failed: {e}. Falling back to CPU.")

        # CPU Fallback
        command_cpu = [
            "ffmpeg", *video_input, *music_input,
            "-filter_complex", filter_complex, "-map", "[outv]", *audio_output_map,
            "-c:v", "libx264", "-preset", "medium", "-crf", "23", "-y", output_path
        ]
        
        print("üöÄ Executing FFmpeg command with CPU rendering...")
        result = subprocess.run(command_cpu, check=True, capture_output=True, text=True, timeout=300)
        print(f"‚úÖ FFmpeg rendering complete with CPU: {output_path}")
        return True, output_path

    except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
        error_output = e.stderr if hasattr(e, 'stderr') else str(e)
        print(f"‚ùå FFmpeg Error: {error_output}")
        return False, f"FFmpeg failed: {error_output[:500]}"
    except Exception as e:
        return False, f"An unexpected error occurred during rendering: {e}"
    finally:
        if ass_path and os.path.exists(ass_path):
            try:
                os.remove(ass_path)
            except OSError as e:
                print(f"Error removing temp subtitle file {ass_path}: {e}")

def process_single_clip(source_video_path, moment, flags, user_id, index):
    base_clip_path = f"temp_clip_{user_id}_{index}.mp4"
    final_clip_path = f"final_clip_{user_id}_{index}.mp4"
    
    try:
        if not utils.cut_video_clip(source_video_path, moment['start'], moment['duration'], base_clip_path):
            raise IOError("Failed to cut base clip.")
            
        words_data = None
        if flags.get('add_captions', False):
            print("Transcribing clip for captions...")
            transcription_result = utils.transcribe_audio_robust(base_clip_path)
            if transcription_result['success']:
                words_data = transcription_result['data']['words']
                if not words_data:
                    print("Warning: Word-level timestamps not found in transcription.")
            else:
                 print(f"Warning: Transcription failed: {transcription_result['error']}")

        success, result = render_clip_with_ffmpeg(
            input_path=base_clip_path, 
            output_path=final_clip_path,
            aspect_ratio=flags.get('aspect', "9:16"), 
            add_captions=flags.get('add_captions', False),
            words_data=words_data, 
            add_music=flags.get('add_music', False)
        )
        
        if success:
            return {'success': True, 'path': result}
        else:
            raise RuntimeError(f"FFmpeg rendering failed: {result}")

    except Exception as e:
        import traceback
        print(f"Error processing clip {index}: {e}")
        print(traceback.format_exc())
        return {'success': False, 'error': str(e)}
    finally:
        if os.path.exists(base_clip_path):
            try:
                os.remove(base_clip_path)
            except OSError as e:
                print(f"Error removing temp clip file {base_clip_path}: {e}")